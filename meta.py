# -*- coding: utf-8 -*-
"""Meta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CF5jo_gc6MYwX9wX3HNuMV3K_PIx-cID

**Meta-Learning using Automated Machine Learning**
"""

!pip install tpot scikit-learn pandas

import tpot
import sklearn
import pandas as pd

print("TPOT version:", tpot.__version__)
print("scikit-learn version:", sklearn.__version__)
print("pandas version:", pd.__version__)

"""**AutoML for Iris Dataset**"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from tpot import TPOTClassifier

# Load the Iris dataset
iris = load_iris()

# Extract features and target
X = iris.data
y = iris.target

# Check the shapes of X and y
print(f"X shape: {X.shape}, y shape: {y.shape}")

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)

# Check the sizes of the resulting sets
print(f'Train set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}')

# Initialize the TPOTClassifier
tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, random_state=42)

# Fit the model to the training data
tpot.fit(X_train, y_train)

# Evaluate the model on the test set
print(f'Test Score: {tpot.score(X_test, y_test)}')

# Export the best pipeline to a .py file
tpot.export('tpot_iris_pipeline.py')

!ls

from google.colab import files
files.download('tpot_iris_pipeline.py')

"""**AutoML for Sonar Dataset**"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tpot import TPOTClassifier

# Load the Iris dataset
sonar_data = pd.read_csv("/content/sonar.csv", header=None)

# Extract features and target
x = sonar_data.drop(columns=60, axis=1)
y = sonar_data[60]

# To convert categorical data into numerical data
y=LabelEncoder().fit_transform(y)

# Check the shapes of X and y
print(f"X shape: {X.shape}, y shape: {y.shape}")

# Split the dataset into training and testing sets
x_train ,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1 ,stratify =  y,random_state = 1)

# Check the sizes of the resulting sets
print(f'Train set size: {x_train.shape[0]}, Test set size: {x_test.shape[0]}')

# Initialize the TPOTClassifier
tpot = TPOTClassifier(generations=10, population_size=20, verbosity=2, random_state=42)

# Fit the model to the training data
tpot.fit(x_train, y_train)

# Evaluate the model on the test set
print(f'Test Score: {tpot.score(x_test, y_test)}')

# Export the best pipeline to a .py file
tpot.export('tpot_sonar_pipeline.py')

prediction=tpot.predict(x_test)
print(prediction)
print(y)

print(tpot.fitted_pipeline_)